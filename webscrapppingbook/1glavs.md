# **Часть I. Разработка веб-скраперов**

В первой части этой книги основное внимание будет уделено
базовым механизмам веб-скрапинга: как на Python построить
запрос к веб-серверу, выполнить базовую обработку
полученного ответа и начать взаимодействие с сайтом в
автоматическом режиме. В итоге вы сможете легко
путешествовать по Интернету, создавая скраперы, способные
переходить от одного домена к другому, собирать и сохранять
информацию для последующего использования. 
Честно говоря, веб-скрапинг — фантастическая отрасль: вложенные в нее относительно небольшие начальные
инвестиции окупятся сторицей. Примерно 90 % проектов веб-скрапинга, которые вам встретятся, будут опираться на
методы, описанные в следующих шести главах. Эта часть
покажет, как обычные (хотя и технически подкованные) люди
представляют себе работу веб-скраперов:

• извлечение HTML-данных из имени домена; 

• анализ этих данных для получения требуемой информации; 

• сохранение этой информации; 

• возможен переход на другую страницу, чтобы повторить процедуру. 

Прочитав эту часть, вы получите прочную основу, которая позволит вам перей ти к более сложным проектам, описанным 
в части II. Не думайте, что первая половина книги менее важна, чем вторая, и более сложные проекты описаны во 
второй половине. При написании веб-скраперов вам придется каждый день использовать почти всю информацию, 
изложенную в первой половине данной книги! 

## **Глава 1. Ваш первый веб-скрапер**

С первых шагов веб-скрапинга вы сразу начнете ценить все те мелкие услуги, которые нам оказывают браузеры. Без 
HTML-форматирования, стилей CSS, скриптов JavaScript и рендеринга изображений Интернет с непривычки может 
показаться слегка пугающим. Но в этой и следующей главах мы посмотрим, как форматировать и интерпретировать 
данные, не прибегая к помощи браузера. 

В этой главе мы начнем с основ отправки на веб-сервер GET-запросов — запросов на выборку или получение 
содержимого заданной веб-страницы, чтения полученного с нее HTML-кода и выполнения ряда простых операций по 
извлечению данных, чтобы выделить оттуда контент, который вы ищете. 

### **Установка соединения**

Если вам прежде не приходилось много работать с сетями или заниматься вопросами сетевой безопасности, то механика 
работы Интернета может показаться несколько загадочной. Вы же не хотите каждый раз задумываться о том, что именно 
делает сеть, когда вы открываете браузер и заходите на сайт **http://google.com**. Впрочем, в наше время это и не 
нужно. Компьютерные интерфейсы стали настолько совершенными, что большинство людей, пользующихся Интернетом, не имеют ни малейшего представления о том, как работает Сеть, и это, по-моему, здорово. Однако веб-скрапинг требует 
сбросить покров с этого интерфейса — на уровне не только браузера (то, как он интерпретирует весь HTML-, CSS- и 
JavaScript-код), но иногда и сетевого подключения. 

Чтобы дать вам представление об инфраструктуре, необходимой для получения информации в браузере, рассмотрим 
следующий пример. У Алисы есть веб-сервер, а у Боба — ПК, с которого он хочет подключиться к серверу Алисы. Когда 
одна машина собирается пообщаться с другой, происходит примерно следующий обмен данными. 

1. Компьютер Боба посылает поток битов — единиц и нулей, которым соответствует высокое и низкое напряжение в кабеле. Из этих битов складывается некая информация, делимая на заголовок и тело. В заголовке содержится MAC-адрес ближайшего локального маршрутизатора и IP-адрес конечной точки — сервера Алисы. В тело включен запрос Боба к серверному приложению Алисы. 

2. Локальный маршрутизатор Боба получает все эти нули и единицы и интерпретирует их как пакет, который передается с MAC-адреса Боба на IP-адрес Алисы. Маршрутизатор Боба ставит свой IP-адрес на пакете в графе «отправитель» и отправляет пакет через Интернет. 

3. Пакет Боба проходит через несколько промежуточных серверов, которые по соответствующим кабелям передают пакет на сервер Алисы. 

4. Сервер Алисы получает пакет по своему IP-адресу. 

5. Сервер Алисы считывает из заголовка пакета номер порта-приемника и передает пакет соответствующему приложению веб-сервера. (Портом-приемником пакета в случае веб-приложений почти всегда является порт номер 80; это словно номер квартиры в адресе для пакетных данных, тогда как IP-адрес аналогичен названию улицы и номеру дома.)

6. Серверное приложение получает поток данных от серверного процессора. Эти данные содержат примерно такую информацию: 

• вид запроса: GET; 

• имя запрошенного файла: index.html. 

7. Веб-сервер находит соответствующий HTML-файл, помещает его в новый пакет, созданный для Боба, и отправляет этот пакет на локальный маршрутизатор, откуда он уже знакомым нам способом передается на компьютер Боба. Вуаля! Так работает Интернет. В чем же при этом обмене данными участвует браузер? Ответ: ни в чем. В действительности браузеры — сравнительно недавнее изобретение в истории Интернета: Nexus появился всего в 1990 году. Конечно, браузер — полезное приложение, которое создает эти информационные пакеты, сообщает операционной системе об их отправке и интерпретирует данные, превращая их в красивые картинки, звуки, видео и текст. Однако браузер — это всего лишь код, который можно разбить на части, выделить основные компоненты, переписать, использовать повторно и приспособить для выполнения чего угодно. Браузер может дать команду процессору отправлять данные в приложение, поддерживающее беспроводной (или проводной) сетевой интерфейс, но то же самое можно сделать и на Python с помощью всего трех строк кода: 

```python
from urllib.request import urlopen 
html = urlopen('http://pythonscraping.com/pages/page1.html') 
print(html.read()) 
```

Для выполнения этого кода можно использовать оболочку iPython, которая размещена в репозитории [GitHub](https://github.com/REMitchell/python-scraping/blob/master/Chapter01_BeginningToScrape.ipynb) для главы 1 , или же сохранить код на компьютере в файле `scrapetest.py` и запустить его в окне терминала с помощью следующей команды: 

```shell
 python scrapetest.py
```

 Обратите внимание: если на вашем компьютере, кроме Python 3.x, также установлен Python 2.x, и вы используете обе 
 версии Python параллельно, то может потребоваться явно вызвать Python 3.x, выполнив команду следующим образом: 

```shell
 python3 scrapetest.py
 ```

 По этой команде выводится полный HTML-код страницы page1, расположенной по [адресу](http://pythonscraping.com/pages/page1.html). Точнее, выводится HTML-файл page1.html, размещенный в каталоге `<корневой веб-каталог>/pages` на сервере с доменным именем _http://pythonscraping.com_. 

/Почему так важно представлять себе эти адреса как /«файлы», а не как «страницы»? /

Большинство современных веб-страниц связано с множеством файлов ресурсов. Ими могут быть файлы изображений, скриптов JavaScript, стилей CSS и любой другой контент, на который ссылается запрашиваемая страница. 

Например, встретив тег `<imgsrc="cuteKitten.jpg">`, браузер знает: чтобы сгенерировать страницу для пользователя, нужно сделать еще один запрос к серверу и получить данные из файла cuteKitten.jpg. Разумеется, у нашего скрипта на Python нет логики, позволяющей вернуться и запросить несколько файлов (пока что) ; он читает только тот HTML-файл, который мы запросили напрямую: `from urllib.request import urlopen` Эта строка  делает именно то, что кажется на первый взгляд: находит модуль Python для запросов (в библиотеке `urllib`) и импортирует оттуда одну функцию — `urlopen`. 

Библиотека `urllib` — это стандартная библиотека Python (другими словами, для запуска данного примера ничего не 
нужно устанавливать дополнительно), в которой содержатся функции для запроса данных через Интернет, обработки файлов cookie и даже изменения метаданных, таких как заголовки и пользовательский программный агент. 

Мы будем активно применять `urllib` в данной книге, так что я рекомендую вам прочитать раздел документации Python, касающийся этой [библиотеки](https://docs.python.org/3/library/urllib.html). Функция `urlopen` открывает 
удаленный объект по сети и читает его. Поскольку это практически универсальная функция (она одинаково легко  читает HTML-файлы, файлы изображений и другие файловые потоки), мы будем довольно часто использовать ее в данной книге. 

### **Знакомство с BeautifulSoup**

> *Прекрасный суп в столовой*
> *ждет.*
> *Из миски жирный пар идет.*
> *Не любит супа тот, кто глуп!*
> *Прекрасный суп, вечерний суп!*
> *Прекрасный суп, вечерний суп!*
> 
> *Алиса в Стране чудес. Издание 1958 г., пер. А. Оленича-Гнененко*

Библиотека **BeautifulSoup** («Прекрасный суп») названа так в честь одноименного стихотворения Льюиса Кэрролла из книги «Алиса в Стране чудес». В книге это стихотворение поет Фальшивая Черепаха (пародия на популярное викторианское
блюдо — фальшивый черепаховый суп, который варят не из черепахи, а из говядины). 

Приложение BeautifulSoup стремится найти смысл в бессмысленном: помогает отформатировать и упорядочить «грязные» сетевые данные, исправляя ошибки в HTML-коде и создавая легко обходимые (traversable) объекты Python, являющиеся представлениями структур XML. 

#### **Установка BeautifulSoup**

Поскольку BeautifulSoup не является стандартной библиотекой Python, ее необходимо установить. Если у вас уже есть опыт установки библиотек Python, то используйте любимый установщик и пропустите этот подраздел, сразу перейдя к
следующему — «Работа с BeautifulSoup» на с. 29. 

Для тех же, кому еще не приходилось устанавливать библиотеки Python (или кто подзабыл, как это делается), представлен общий подход, который мы будем использовать
для установки библиотек по всей книге, так что впоследствии вы можете обращаться к данному подразделу. 

В этой книге мы будем использовать библиотеку
*BeautifulSoup* 4 (также известную как *BS4).* 

Подробная инструкция по установке *BeautifulSoup4* размещена на сайте [Crummy.com](http://www.crummy.com/software/BeautifulSoup/bs4/doc/); здесьже описан самый простой способ для Linux:

```shell
sudo apt-get install python-bs4
```
Эта команда устанавливает менеджер пакетов Python *pip*. 
Затем выполните следующую команду для установки библиотеки:

```shell
pip install beautifulsoup4
```
При использовании pip с целью установки пакета для Python 3.x также можно вызвать pip3:

```shell
pip3 install beautifulsoup4
```

Готово! Теперь компьютер будет распознавать *BeautifulSoup* как библиотеку Python. Чтобы в этом убедиться, откройте терминал Python и импортируйте ее:

```python
from bs4 import BeautifulSoup
```

### **Работа с BeautifulSoup**

Из всех объектов библиотеки *BeautifulSoup* чаще всего
используется собственно, сам *BeautifulSoup.* Посмотрим, как
он работает, изменив пример, приведенный в начале главы: 

```python
from urllib.request import urlopen
from bs4 import BeautifulSoup
html = urlopen('http://www.pythonscrapin.com/pages/page1.html')
bs = BeautifulSoup(html.read(), 'html.parser')
print(bs.h1)
```

*Результат выглядит так:*

`<h1>An Interesting Title</h1>`

Обратите внимание: этот код возвращает только первый попавшийся ему на странице экземпляр тега `h1.` По 
существующему соглашению на странице может быть только один тег `h1,` *однако принятые в Интернете соглашения 
часто нарушаются*. Поэтому следует помнить, что таким образом будет получен только первый экземпляр тега и не 
обязательно тот, который вы ищете. 

Как и в предыдущих примерах веб-скрапинга, мы импортируем функцию `urlopen` и вызываем `html.read()`, чтобы 
получить контент страницы в формате HTML. Помимо текстовой строки, *BeautifulSoup* также может принимать файловый 
объект, непосредственно возвращаемый функцией `urlopen.` Чтобы получить этот объект, не нужно вызывать функцию 
`.read():`

`bs = BeautifulSoup(html, 'html.parser')`

Здесь контент HTML-файла преобразуется в объект *BeautifulSoup,* имеющий следующую структуру:

```html
html ** 
<html><head>...</head><body>...</body> 
</html> 
![Image 8](images/000014.png)
![Image 9](images/000042.png)
![Image 10](images/000071.png)
![Image 11](images/000021.png)
![Image 12](images/000050.png)
![Image 13](images/000013.png)
![Image 14](images/000027.png)
— *head ** <head><title>A Useful Page<title> 
</head> 
— *title ** <title>A Useful Page</title> 
— *body ** <body><h1>An Int...</h1><div>Lorem ip...</div></body> 
— *h1 ** <h1>An Interesting Title</h1> 
— *div ** <div>Lorem Ipsum dolor...</div> 
```

Обратите внимание: тег `h1,` извлеченный из кода страницы, находится на втором уровне структуры объекта 
`BeautifulSoup(html body h1)` Однако, извлекая `h1` из объекта, мы обращаемся к этому тегу напрямую: `bs.h1` На 
практике все следующие вызовы функций приведут к одинаковым результатам:

```python
bs.html.body.h1
bs.body.h1
bs.html.h1
```
При создании объекта `BeautifulSoup` функции передаются два аргумента: `bs = BeautifulSoup(html.read(),'html.parser')` Первый аргумент — это текст в формате HTML, на основе которого строится объект, а второй — 
синтаксический анализатор, который `BeautifulSoup` будет использовать для построения объекта. В большинстве случаев 
не имеет значения, какой именно синтаксический анализатор будет применяться. Анализатор html.parser входит в 
состав Python 3 и не требует дополнительной настройки перед использованием. За редким исключением, в этой книге я
 буду применять именно его. Еще один популярный анализатор 
— 

[lxml](http://lxml.de/parsing.html). Он устанавливается через pip: 

```shell
pip3 install lxml
```

Для того чтобы использовать lxml в BeautifulSoup, нужно изменить имя синтаксического анализатора в знакомой
нам строке:

```python
bs = BeautifulSoup(html.read(), 'lxml')
```

Преимущество `lxml`, по сравнению с `html.parser`, состоит в том, что `lxml` в целом лучше справляется с «грязным» или 
искаженным HTML-кодом. Анализатор `lxml` прощает неточности и исправляет такие проблемы, как незакрытые и 
неправильно вложенные теги, а также отсутствующие теги `head` или `body`. Кроме того, `lxml` работает несколько 
быстрее, чем `html.parser`, хотя при веб-скрапинге скорость анализатора не всегда является преимуществом, поскольку
 почти всегда *главное узкое место — скорость самого сетевого соединения*.
 Недостатками анализатора `lxml` является то, что его необходимо специально устанавливать и он зависит от сторонних C-библиотек.  
 Это может вызвать проблемы портируемости; кроме того, `html.parser` проще в использовании. 
 
 Еще один популярный синтаксический анализатор HTML называется `html5lib`. Подобно `lxml`, он чрезвычайно лоялен к ошибкам и 
 прилагает еще больше усилий к исправлению некорректного HTML-кода. Он также имеет внешние зависимости и работает медленнее, чем `lxml` и html.parser. 
 
 Тем не менее выбор `html5lib` может быть оправданным *при работе с «грязными» или написанными вручную 
 HTML-страницами*. Чтобы использовать этот анализатор, нужно установить его
 и передать объекту BeautifulSoup строку html5lib: 
 
 ```python
 bs = BeautifulSoup(html.read(), 'html5lib')
 ```

 Надеюсь,благодаря этой краткой дегустации BeautifulSoup вы составили представление о возможностях и удобстве данной 
 библиотеки. В сущности, *она позволяет извлечь любую информацию из любого файла в формате HTML (или XML)*, если 
 содержимое данного файла заключено в идентифицирующий тег или этот тег хотя бы присутствует в принципе. В главе 
 2 мы подробно рассмотрим более сложные вызовы функций библиотеки, а также регулярные выражения и способы их 
 использования с помощью BeautifulSoup для извлечения информации с сайтов. 

### **Надежное соединение и обработка исключений**

Сеть — «грязное» место. Данные плохо отформатированы, сайты то и дело «падают», а разработчики страниц постоянно забывают ставить закрывающие теги. Один из самых
неприятных моментов, связанных с веб-скрапингом, — уйти спать и оставить работающий скрапер, рассчитывая назавтра иметь все данные в вашей базе, а утром обнаружить, что скрапер столкнулся с ошибкой в каком-то непредсказуемом
формате данных и почти сразу прекратил работу, стоило вам отвернуться от экрана. В подобных случаях возникает соблазн проклясть разработчика, который создал тот сайт (и выбрал
странный формат представления данных). Однако на самом деле если кому и стоит дать пинка, то это вам самим, поскольку именно вы не предусмотрели исключение! 

Рассмотрим первую строку нашего скрапера, сразу после операторов импорта, и подумаем, как можно обрабатывать любые исключения, которые могли бы здесь возникнуть: 

`html = urlopen('http://www.pythonscraping.com/pages/page1.html')`

Здесь могут случиться две основные неприятности:

• на сервере нет такой страницы (или при ее получении произошла ошибка); 

• нет такого сервера. 

В первой ситуации будет возвращена ошибка HTTP. Это может быть 404 Page Not Found, 500 Internal Server Error и т.п. 

Во всех таких случаях функция `urlopen` выдаст обобщенное исключение `HTTPError`. Его можно обработать следующим образом:

```python
from urllib.request import urlopen
from urllib.error import HTTPError

try:
	html = urlopen('http://www.pythonscraping.com/pages/page1.html')

except HTTPError as e:
	print(e)
	# Вернуть ноль, прекратить работу или
	# выполнить еще какой-нибудь "план Б". 
else:
	# Продолжить выполнение программы. 
	# Примечание: если при перехвате исключений
	# программа прерывает работу или происходит возврат
	# из функции, то оператор else не нужен. 
```

Теперь в случае возвращения кода HTTP-ошибки выводится сообщение о ней и остальная часть программы, которая находится в ветви `else`, не выполняется. Если не 
найден весь сервер (например, сервер по адресу http://www.pythonscraping.com отключен или URL указан с ошибкой), то функция `urlopen` возвращает `URLError`. 
Эта ошибка говорит о том, что ни один из указанных серверов не доступен. Поскольку именно удаленный сервер отвечает за возвращение кодов состояния `HTTP`, 
ошибка `HTTPError` не может быть выдана и вместо нее следует обрабатывать более серьезную ошибку `URLError`. 

Для этого можно добавить в программу такую проверку:

```python
from urllib.request import urlopen
from urllib.error import HTTPError
from urllib.error import URLError

try:
	html = urlopen('https://pythonscrapingthisurldoesnotexist.com')

except HTTPError as e:
	print(e)

except URLError as e:
	print('The server could not be found!')

else:
	print('It Worked!')
```

Конечно, даже если страница успешно получена с сервера, все равно остается проблема с ее контентом, который не всегда соответствует ожидаемому. Всякий раз, 
обращаясь к тегу в объекте BeautifulSoup, разумно добавить проверку того, существует ли этот тег. При попытке доступа к несуществующему тегу BeautifulSoup 
возвращает объект `None`. Проблема в том, что попытка обратиться к тегу самого объекта `None` приводит к возникновению ошибки `AttributeError`. Следующая
 строка (в которой `nonExistentTag` — несуществующий тег, а не имя реальной функции BeautifulSoup) возвращает объект None: `print(bs.nonExistentTag)` Этот 
 объект вполне доступен для обработки и проверки. Проблема возникает в том случае, если продолжать его использовать без проверки и попытаться вызвать для 
 объекта None другую функцию: `print(bs.nonExistentTag.someTag)` Эта функция вернет исключение:` AttributeError: 'NoneType' object has no attribute 'someTag' `Как же застраховаться от этих ситуаций? Проще всего — явно проверить обе ситуации: 
 try: badContent = bs.nonExistingTag.anotherTag 
 except AttributeError as e: print('Tag was not found') 
 else: if badContent == None: print ('Tag was not found') 
 else: print(badContent)
  Такие проверка и обработка каждой ошибки поначалу могут показаться трудоемкими, однако если немного упорядочить код, то его станет проще писать (и, что еще важнее, гораздо проще читать). 
  
  *Вот, например, все тот же наш скрапер, написанный немного по-другому:*

```python
from urllib.request import urlopen

from urllib.error import HTTPError

from bs4 import BeautifulSoup

def getTitle(url):

	try:
		html = urlopen(url)

	except HTTPError as e:
		return None

	try:
		bs = BeautifulSoup(html.read(), 'html.parser')
		title = bs.body.h1

	except AttributeError as e:
		return None

	return title

title = getTitle('http://www.pythonscraping.com/pages/page1.html')

if title == None:
	print('Title could not be found')
else:
	print(title)
```

В этом примере мы создаем функцию `getTitle`, которая возвращает либо заголовок страницы, либо, если получить его не удалось, — объект None. Внутри getTitle 
мы, как в предыдущем примере, проверяем наличие HTTPError и инкапсулируем две строки BeautifulSoup внутри оператора try. Ошибка AttributeError может 
возникнуть в любой из этих строк (если сервер не найден, то html вернет объект None, а html.read() выдаст AttributeError). Фактически внутри оператора try 
можно разместить любое количество строк или вообще вызвать другую функцию, которая будет генерировать AttributeError в любой момент. 

*При написании скраперов важно продумать общий шаблон кода, который бы обрабатывал исключения, но при этом был бы читабельным. Вы также, вероятно, захотите 
использовать код многократно. Наличие обобщенных функций, таких как getSiteHTML и getTitle (в сочетании с тщательной обработкой исключений), позволяет быстро 
— и надежно — собирать данные с веб-страниц в Сети.* 